{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Nf74B2tbcnYh"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nassma2019/PracticalSessions/blob/master/rnn/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfIdnkh92358"
      },
      "source": [
        "# RNN training tutorial\n",
        "\n",
        "This is a tutorial training various RNNs on simple datasets and doing some analysis.\n",
        "\n",
        "Structure:\n",
        "\n",
        "  1. basic (vanilla RNN) implementation\n",
        "  2. observing exploding/vanishing gradients\n",
        "  3. Intepretability by plotting and analysing activations of a network:\n",
        "    * identifying interpretable neurons\n",
        "    * identifying neurons-gates interactions\n",
        "    * identifying hidden state dynamics through time\n",
        "  4. training an LSTM on character level language modelling task\n",
        "    * comparing training of an LSTM and RNN, playing with architectures\n",
        "    \n",
        "\n",
        "First three sections are almost independent, one can go switch between them without any code dependencies (apart from being unable to use vanilla RNN in section 4, if it was not implemented in 1.).\n",
        "\n",
        "Cells that include \"starting point\" in their title require filling in some code gaps; all remaining ones are complete (but feel free to play with them if you want!)\n",
        "\n",
        "Please pay attention to questions after each section. Finding out answers to these is crucial to make sure one understands various modes of RNN operation.\n",
        "\n",
        "Language model exercises are based on [Sonnet LSTM example](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/rnn_shakespeare.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "HspsT1GJr_Yc",
        "colab": {}
      },
      "source": [
        "#@title Installing Sonnet\n",
        "!pip install -q dm-sonnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MA_K3_OL3EY4"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We will use Sonnet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "j5YGV2hb2RIt",
        "colab": {}
      },
      "source": [
        "#@title imports\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sonnet.examples import dataset_shakespeare\n",
        "  \n",
        "sns.set_style('ticks')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W0KPZdiq5AVJ"
      },
      "source": [
        "# Ex 1.    Vanilla RNN\n",
        "\n",
        "Implement basic RNN cell in sonnet.\n",
        "\n",
        "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
        "   \n",
        "   Where\n",
        "   \n",
        "   * $x_t$ input at time $t$\n",
        "   * $h_t$ hidden state at time $t$\n",
        "   * $W$ input-to-hidden mapping (trainable)\n",
        "   * $V$ hidden-to-hidden mapping (trainable)\n",
        "   * $b$ bias (trainable)\n",
        "   * $f$ non-linearity chosen (usually tanh)\n",
        "   \n",
        "   \n",
        "   You do not need to worry about the plotting and running code, but focus on the RNN implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "LlnUeaf-6zUY",
        "colab": {}
      },
      "source": [
        "#@title Vanilla RNN - Starting point\n",
        "class RNN(snt.RNNCore):\n",
        "  \n",
        "  def __init__(self, hidden_size, activation=tf.tanh, name=\"vanilla_rnn\"):    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "    \n",
        "      hidden_size: number of hidden units\n",
        "      activation: function constructing activation op\n",
        "      name: name of the core\n",
        "    \"\"\"    \n",
        "    # We have to call parent's constructor with the name provided\n",
        "    super(RNN, self).__init__(name=name)\n",
        "    self._hidden_size = hidden_size\n",
        "    self._activation = activation\n",
        "\n",
        "  def _build(self, input_, prev_state):\n",
        "    \"\"\"\n",
        "    This function will be called in a loop, when RNN core is connected to\n",
        "    ops creating inputs and previous states.\n",
        "    \n",
        "    Args:\n",
        "    \n",
        "      input_: tensor containing current x_t\n",
        "      prev_state: tensor containing previous state, h_{t-1}\n",
        "    \"\"\"\n",
        "    \n",
        "    ################\n",
        "    #   YOUR CODE  #\n",
        "    ################\n",
        "    \n",
        "    # Cores in sonnet return pairs of (o_t, h_t) where o_t is the output\n",
        "    # exposed to the rest of the code, for Vanilla RNN these are the same\n",
        "    # quantities\n",
        "    return output, output\n",
        "  \n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])\n",
        "\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "z6GIHgOnzd8Y",
        "colab": {}
      },
      "source": [
        "#@title Vanilla RNN - Solution\n",
        "class RNN(snt.RNNCore):\n",
        "  \n",
        "  def __init__(self, hidden_size, activation=tf.tanh, name=\"vanilla_rnn\"):    \n",
        "    \n",
        "    super(RNN, self).__init__(name=name)\n",
        "    self._hidden_size = hidden_size\n",
        "    self._activation = activation    \n",
        "    \n",
        "\n",
        "  def _build(self, input_, prev_state):\n",
        "    self._in_to_hidden_linear = snt.Linear(\n",
        "        self._hidden_size, name=\"in_to_hidden\")\n",
        "\n",
        "    self._hidden_to_hidden_linear = snt.Linear(\n",
        "        self._hidden_size, name=\"hidden_to_hidden\")\n",
        "    \n",
        "    in_to_hidden = self._in_to_hidden_linear(input_)\n",
        "    hidden_to_hidden = self._hidden_to_hidden_linear(prev_state)\n",
        "    output = self._activation(in_to_hidden + hidden_to_hidden)\n",
        "\n",
        "    return output, output\n",
        "  \n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])\n",
        "\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-jCR9YGaI7my"
      },
      "source": [
        "### Train the RNN\n",
        "\n",
        "Train the RNN on sine data - predict the next sine value from predicted sine values.\n",
        "\n",
        "Predict   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
        "\n",
        "To learn the prediction model, we will use *teacher forcing*. This means that when training the model, the input at time $t$ is the real sequence at time $t$, rather than the output produced by the model at $t-1$. When we want to generate data from the model, we do not have access to the true sequence, so we do not use teacher forcing. However, in the case of our problem, we will also use *warm starting*, because we require multiple time steps to predict the next sine wave value (at least 2, for the initial value and for the step). \n",
        "\n",
        "The code below unrolls the RNN core you have defined above, does the training using backprop though time and plots the real data (\"ground truth\"), the data generated during training (\"train predictions\") and the model samples \"generated\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KurAORYC3MFB",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Training parameters.\n",
        "UNROLL_LENGTH = 30 #@param\n",
        "NUM_ITERATIONS = 10000#@param\n",
        "WARM_START = 10#@param\n",
        "TEACHER_FORCING = True#@param\n",
        "HIDDEN_UNITS = 20 #@param\n",
        "LEARNING_RATE = 0.0005 #@param\n",
        "REPORTING_INTERVAL = 1000 #@param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "paHrijB1kbiQ",
        "colab": {}
      },
      "source": [
        "# Training a simple sequence, optionally using teacher forcing and sampling.\n",
        "\n",
        "# We create training data, sine wave over [0, 2pi]\n",
        "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1, 1)\n",
        "y_train = np.sin(x_train)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Creating two sonnet modules, one - an RNN we defined in the previous cell\n",
        "# and the second - a linear mapping from RNN output to the target - scalar\n",
        "rnn = RNN(HIDDEN_UNITS)\n",
        "dec = snt.Linear(1)\n",
        "\n",
        "# Placeholder is gonna be used to provide a subsequence on which we will learn\n",
        "# Each subsequence will be a consequitive block of UNROLL_LENGTH values of\n",
        "# the sine wave\n",
        "sequence = tf.placeholder(tf.float32, [UNROLL_LENGTH, 1, 1])\n",
        "\n",
        "\n",
        "# First, the training section\n",
        "\n",
        "losses = []\n",
        "train_predictions = []\n",
        "current_state = rnn.initial_state(1)\n",
        "\n",
        "# For simplicity, we will unroll our RNN by hand in a loop\n",
        "for i in range(UNROLL_LENGTH-1):\n",
        "  \n",
        "  # In teacher forcing setup, input is the true previous output\n",
        "  if TEACHER_FORCING:\n",
        "    input_ = sequence[i]\n",
        "  else:\n",
        "    # In \"generative\" mode the input is our own previous prediction\n",
        "    if i <= WARM_START:\n",
        "      # We can still use teacher forcing at the very beginning of training\n",
        "      input_ = sequence[i]\n",
        "    else:\n",
        "      input_ = prediction\n",
        "      \n",
        "  # Connecting an RNN, we apply our cell to (input, state) pair, and we get\n",
        "  # (output, next_state) pair in return\n",
        "  output, current_state = rnn(input_, current_state)\n",
        "  \n",
        "  prediction = dec(output)\n",
        "  losses.append(tf.reduce_sum((prediction - sequence[i+1])**2))\n",
        "  train_predictions.append(prediction)\n",
        "\n",
        "loss = tf.add_n(losses) / len(losses)\n",
        "train_predictions = tf.stack(train_predictions)\n",
        "  \n",
        "opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
        "train_op = opt.minimize(loss)\n",
        "\n",
        "\n",
        "# Now we define a part of the graph used for generating data\n",
        "# The code is almost the same as the training one, with the only difference\n",
        "# being that now we unroll over entire signal, and use our own predictions\n",
        "# to condition ourselves.\n",
        "\n",
        "current_state = rnn.initial_state(1)\n",
        "predictions = []\n",
        "sampling_losses = []\n",
        "for i in range(len(y_train)-1):\n",
        "  if i <= WARM_START:\n",
        "    input_ = tf.constant(y_train[i], dtype=tf.float32)\n",
        "  else:\n",
        "    input_ = prediction\n",
        "  output, current_state = rnn(input_, current_state)\n",
        "  prediction = dec(output)\n",
        "  sampling_losses.append(tf.reduce_sum((prediction - y_train[i+1])**2))\n",
        "  predictions.append(prediction)\n",
        "\n",
        "sampling_loss = tf.add_n(sampling_losses) / len(sampling_losses)\n",
        "predictions = tf.stack(predictions)\n",
        "\n",
        "\n",
        "# At this point entire graph is built, and we can create the session and run\n",
        "\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  for i in range(NUM_ITERATIONS):\n",
        "    start = np.random.choice(\n",
        "      range(x_train.shape[0] - UNROLL_LENGTH)\n",
        "    )\n",
        "    l, p, _ = sess.run([loss, train_predictions, train_op], \n",
        "                    feed_dict={sequence: y_train[start: start+UNROLL_LENGTH]})\n",
        "    \n",
        "    if i % REPORTING_INTERVAL == 0:\n",
        "      pred, pred_l = sess.run([predictions, sampling_loss])\n",
        "      plt.figure()\n",
        "      plt.title('Training Loss %f;  Sampling loss %f' % (l, pred_l))\n",
        "      plt.plot(pred.ravel(), c='r', label='Generated')\n",
        "      plt.plot(y_train[1:].ravel(), c='b', label='Ground truth')\n",
        "      plt.plot(range(start, start+UNROLL_LENGTH-1), p.ravel(), c='g',\n",
        "               label='Train prediction')\n",
        "      plt.legend()\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WLuIAK8LJWay"
      },
      "source": [
        "###What is worth trying/understanding here?\n",
        "\n",
        "* Difference between teacher forcing and learning on own samples\n",
        " * What is teacher forcing really learning?\n",
        " * Why is the model struggling to learn in one of the setups?\n",
        " * What is it we actually care about for models like this? What should be the actual surrogate?\n",
        "* How does warm starting affect our training? Why?\n",
        "* What happens if the structure of interest is much longer than the unroll length?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uxyUegmC5_Hj"
      },
      "source": [
        "# Ex. 2      Vanishing and exploding gradients\n",
        "\n",
        "Given a sequence $(x_1, ..., x_N)$ of random floats (sampled from normal distribution), compute:\n",
        "$$\n",
        "\\left \\| \\frac{\\partial h_{N}}{\\partial h_i} \\right \\|\n",
        "$$\n",
        "for each $i$, and plot these quantities for various RNNs.\n",
        "\n",
        "Note, that during learning one would compute\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\theta}  \n",
        "$$\n",
        "which, using chain rule will involve terms like\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial h_N} \\cdot\n",
        "\\frac{\\partial h_N}{\\partial h_i} \\cdot\n",
        "\\frac{\\partial h_i}{\\partial \\theta}\n",
        "$$\n",
        "so if one of them vanishes, all of them do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "8FGIuk16S6M-",
        "colab": {}
      },
      "source": [
        "#@title Vanishing and exploding gradients - Starting point\n",
        "tf.reset_default_graph()\n",
        "\n",
        "SEQ_LENGTH = 15 #@param\n",
        "HIDDEN_UNITS = 30 #@param\n",
        "\n",
        "# We are gonna use completely random signal as input\n",
        "dummy_input = [tf.constant([[np.random.normal()]]) for _ in range(SEQ_LENGTH)] \n",
        "\n",
        "rnn_types = {\n",
        "    'LSTM': (lambda x: snt.LSTM(x, forget_bias=1), lambda x: x.cell),\n",
        "    'RNN': (RNN, lambda x: x)\n",
        "\n",
        "    # OPTIONAL.\n",
        "    # Add more cores here, by playing with forget_bias and initializers\n",
        "    # of LSTMs, can you create a core which is initialised to having\n",
        "    # forget_gate always open (no forgetting) and input gate closed\n",
        "    # (no dependence on input)?\n",
        "    \n",
        "    ##########################\n",
        "    #  (OPTIONAL) YOUR CODE  #\n",
        "    ##########################\n",
        "}\n",
        "\n",
        "depths = {rnn_type: [] for rnn_type in rnn_types}\n",
        "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
        "\n",
        "# Iterate over all RNNs we want to analyse\n",
        "for rnn_type in rnn_types:\n",
        "  \n",
        "  # Each entry is a pair (constructor, extractor)\n",
        "  constructor, extractor = rnn_types[rnn_type]\n",
        "  \n",
        "  # extractor is a function that will return vector of interest when applied to state\n",
        "  \n",
        "  rnn = constructor(HIDDEN_UNITS)\n",
        "\n",
        "  rnn_at_time = []\n",
        "  gradients_at_time = []\n",
        "\n",
        "  # We need to manually unroll our RNNs such that they populate:\n",
        "  #  rnn_at_time[t] - extractor applied to state at time t\n",
        "  \n",
        "  ################\n",
        "  #   YOUR CODE  #\n",
        "  ################\n",
        "  \n",
        "  # Dummy loss is just a sum of activations, as we are not training anything\n",
        "  # and just need an anchor for partial derivatives computation\n",
        "  dummy_loss = tf.reduce_sum(rnn_at_time[-1])\n",
        "  \n",
        "  # We use tf.gradients to compute partial derivatives of interest\n",
        "  for i in range(1, SEQ_LENGTH):\n",
        "    current_gradient = tf.gradients(\n",
        "      dummy_loss, \n",
        "      rnn_at_time[i],   \n",
        "    )\n",
        "    gradients_at_time.append(current_gradient)\n",
        "  \n",
        "  # Finally, compute actual gradients by calling our tf.gradients ops\n",
        "  init = tf.global_variables_initializer()  \n",
        "  with tf.train.SingularMonitoredSession() as sess:\n",
        "    sess.run(init)\n",
        "    gradients = sess.run(gradients_at_time)\n",
        "\n",
        "  # Compute norms of gradients  \n",
        "  for gid, grad in enumerate(gradients):\n",
        "    depths[rnn_type].append(len(gradients)-gid)    \n",
        "    grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
        "\n",
        "plt.figure()\n",
        "for rnn_type in depths:\n",
        "  plt.plot(depths[rnn_type], grad_norms[rnn_type],\n",
        "           label=\"%s\" % rnn_type, alpha=0.7)\n",
        "plt.legend()  \n",
        "# Matplotlib supports latex!\n",
        "plt.ylabel(\"$ \\\\| \\\\partial \\\\sum_i {c_{N}}_i / \\\\partial c_t \\\\|$\", fontsize=15)\n",
        "plt.xlabel(\"Steps through time - $t$\", fontsize=15)\n",
        "plt.xlim((1, SEQ_LENGTH-1))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "zljqN01vc9-3",
        "colab": {}
      },
      "source": [
        "#@title Vanishing and exploding gradients - Solution\n",
        "tf.reset_default_graph()\n",
        "\n",
        "SEQ_LENGTH = 20 #@param\n",
        "HIDDEN_UNITS = 30 #@param\n",
        "\n",
        "dummy_input = [tf.constant([[np.random.normal()]]) for _ in range(SEQ_LENGTH)] \n",
        "\n",
        "rnn_types = {\n",
        "    'LSTM fixed gates': (lambda x: snt.LSTM(x, forget_bias=1000, \n",
        "                                    initializers={\n",
        "                                        # bias of gates.\n",
        "                                        'b_gates': tf.constant_initializer(-100.),\n",
        "                                    }), lambda x: x.cell),\n",
        "    'LSTM 0': (lambda x: snt.LSTM(x, forget_bias=0), lambda x: x.cell),\n",
        "    'LSTM +1': (lambda x: snt.LSTM(x, forget_bias=1), lambda x: x.cell),\n",
        "    'LSTM -2': (lambda x: snt.LSTM(x, forget_bias=-2), lambda x: x.cell),\n",
        "    'LSTM +2': (lambda x: snt.LSTM(x, forget_bias=2), lambda x: x.cell),\n",
        "    'GRU z+5': (lambda x: snt.GRU(x, \n",
        "                                  initializers={\n",
        "                                      # bias for update_cell.\n",
        "                                      'bz': tf.constant_initializer(-5.)\n",
        "                                  }), lambda x: x),\n",
        "    'RNN': (RNN, lambda x: x)\n",
        "}\n",
        "\n",
        "depths = {rnn_type: [] for rnn_type in rnn_types}\n",
        "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
        "\n",
        "for rnn_type in rnn_types:\n",
        "  \n",
        "  constructor, extractor = rnn_types[rnn_type]\n",
        "  \n",
        "  rnn = constructor(HIDDEN_UNITS)\n",
        "\n",
        "  rnn_at_time = []\n",
        "  gradients_at_time = []\n",
        "\n",
        "  prev_state = rnn.initial_state(1)\n",
        "  \n",
        "  for i in range(SEQ_LENGTH):\n",
        "    _, prev_state = rnn(\n",
        "      dummy_input[i], prev_state\n",
        "    )\n",
        "    rnn_at_time.append(extractor(prev_state))\n",
        "\n",
        "  dummy_loss = tf.reduce_sum(rnn_at_time[-1])\n",
        "  \n",
        "  for i in range(1, SEQ_LENGTH):\n",
        "    current_gradient = tf.gradients(\n",
        "      dummy_loss, \n",
        "      rnn_at_time[i],   \n",
        "    )\n",
        "    gradients_at_time.append(current_gradient)\n",
        "  \n",
        "  init = tf.global_variables_initializer()  \n",
        "  with tf.train.SingularMonitoredSession() as sess:\n",
        "    sess.run(init)\n",
        "    gradients = sess.run(gradients_at_time)\n",
        "\n",
        "  for gid, grad in enumerate(gradients):\n",
        "    depths[rnn_type].append(len(gradients)-gid)    \n",
        "    grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
        "\n",
        "plt.figure()\n",
        "for rnn_type in depths:\n",
        "  plt.plot(depths[rnn_type], grad_norms[rnn_type],\n",
        "           label=\"%s\" % rnn_type, alpha=0.7)\n",
        "plt.legend()  \n",
        "plt.ylabel(\"$ \\\\| \\\\partial \\\\sum_i {c_{N}}_i / \\\\partial c_t \\\\|$\", fontsize=15)\n",
        "plt.xlabel(\"Steps through time - $t$\", fontsize=15)\n",
        "plt.xlim((1, SEQ_LENGTH-1))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-4v7TUtjKHD-"
      },
      "source": [
        "### What do we learn from this?\n",
        "\n",
        "This particular experiment is an extremely simple surrogate for actual problem, but shows a few interesting aspects:\n",
        "\n",
        "* Is LSTM by construction free of *exploding* gradients too?\n",
        "* What are other ways of avoiding explosions you can think of?\n",
        "* Does initialisation (of gates here, but in general) matter a lot?\n",
        "* Does this look like a solution that can really scale time-wise? Say to be doing credit assignment through years of experience? If not, what might be a next step?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nf74B2tbcnYh"
      },
      "source": [
        "# [OPTIONAL] Ex. 3    Language Modelling\n",
        "\n",
        "Now we will train a character level RNN on text data - specifically Shakespeare sonnets. We will reuse the same concepts, such as teacher forcing and different types of RNN cores. \n",
        "\n",
        "At the end of the exercise, after you have filled in the TextModel class, you can train the model and see that in generates text that has sonnet structure and learns words.  You should focus on the TextModel class implementation, and leverage the code provided to do the training and visualization and data loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "Xmr1EjIQAM95",
        "colab": {}
      },
      "source": [
        "#@title Dataset wrapper\n",
        "\n",
        "class TinyShakespeare(dataset_shakespeare.TinyShakespeareDataset):\n",
        "  \n",
        "  def _find_starts(self):\n",
        "    starts = []\n",
        "    code = self._data_source._vocab_dict['|']\n",
        "    for i in range(len(self._flat_data)-4):\n",
        "      if code == self._flat_data[i] == self._flat_data[i+1]:\n",
        "        starts.append(i+2)\n",
        "    return np.array(starts)\n",
        "  \n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(TinyShakespeare, self).__init__(*args, **kwargs)\n",
        "    self._starts = self._find_starts()\n",
        "    self._reset_head_indices()\n",
        "  \n",
        "  def cost(self, logits, target):\n",
        "    return super(TinyShakespeare, self).cost(logits, target) / self._num_steps\n",
        "  \n",
        "  def _reset_head_indices(self):\n",
        "    try:\n",
        "      self._head_indices = self._starts[np.random.randint(\n",
        "          low=0, high=len(self._starts), size=[self._batch_size])]\n",
        "    except:\n",
        "      self._head_indices = np.zeros(self._batch_size)\n",
        "      \n",
        "  def to_human_readable(self,\n",
        "                      data,\n",
        "                      label_batch_entries=True,\n",
        "                      indices=None,\n",
        "                      sep=\"\\n\",\n",
        "                      pretify=False):\n",
        "    new_data = super(TinyShakespeare, self).to_human_readable(data, \n",
        "                                                              label_batch_entries,\n",
        "                                                              indices, sep)\n",
        "    if pretify:\n",
        "      new_data = \"    \" + new_data.replace(\"|\", \"\\n    \")\n",
        "    return new_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "8hZVt_hIJsX1",
        "colab": {}
      },
      "source": [
        "#@title Playing with the dataset\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Create sonnet dataset object\n",
        "dataset_train = TinyShakespeare(\n",
        "  num_steps=64,\n",
        "  batch_size=1,\n",
        "  subset=\"train\",\n",
        "  random=False,\n",
        "  name=\"shake_train\")\n",
        "\n",
        "# Create TF ops to read sequences and their corresponding targets\n",
        "train_input_sequence, train_target_sequence = dataset_train()\n",
        "\n",
        "\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  \n",
        "  for k in range(1, 4):\n",
        "    sampled = sess.run(train_input_sequence)\n",
        "    if k == 1:\n",
        "      print(\"Network-friendly data\")\n",
        "      print(\"Data type\", type(sampled))\n",
        "      print(\"Data shape\", sampled.shape)\n",
        "      print(sampled)\n",
        "    print()\n",
        "    print(\"Iteration %d\" % k)\n",
        "    print(dataset_train.to_human_readable((sampled,),))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "n-4wwC9ASqPK",
        "colab": {}
      },
      "source": [
        "#@title TextModel - starting point\n",
        "\n",
        "class TextModel(snt.AbstractModule):\n",
        "  \"\"\"A deep RNN model, for use on the Tiny Shakespeare dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, num_hidden, depth, output_size, embedding_size=32,\n",
        "               rnn_core=RNN, name=\"text_model\"):\n",
        "    \"\"\"Constructs a `TextModel`.\n",
        "    Args:\n",
        "      num_hidden: Number of hidden units in each LSTM layer.\n",
        "      depth: Number of RNN layers.\n",
        "      output_size: Size of the output layer on top of the DeepRNN.\n",
        "      rnn_core: Reference to an RNNCore class such as LSTM.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "\n",
        "    super(TextModel, self).__init__(name=name)\n",
        "\n",
        "    self._num_hidden = num_hidden\n",
        "    self._depth = depth\n",
        "    self._output_size = output_size\n",
        "    self._rnn_core = rnn_core    \n",
        "    \n",
        "    with self._enter_variable_scope():\n",
        "      self._embed = snt.nets.MLP([embedding_size], activate_final=True)\n",
        "      self._output_module = snt.Linear(self._output_size, name=\"linear_output\")\n",
        "      self._subcores = [\n",
        "          self._rnn_core(self._num_hidden, name=\"rnn_{}\".format(i))\n",
        "          for i in range(self._depth)\n",
        "      ]    \n",
        "      self._core = snt.DeepRNN(self._subcores,\n",
        "                               skip_connections=False,\n",
        "                               name=\"deep_rnn\")\n",
        "\n",
        "  def _build(self, one_hot_input_sequence):\n",
        "    \"\"\"Builds the deep RNN model sub-graph.\n",
        "    Args:\n",
        "      one_hot_input_sequence: A Tensor with the input sequence encoded as a\n",
        "        one-hot representation. Its dimensions should be `[truncation_length,\n",
        "        batch_size, output_size]`.\n",
        "    Returns:\n",
        "      Tuple of the Tensor of output logits for the batch, with dimensions\n",
        "      `[truncation_length, batch_size, output_size]`, and the\n",
        "      final state of the unrolled core,.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unroll the RNN core, consider using tf.contrib.rnn.static_rnn\n",
        "    # Note that you will need to use snt.BatchApply to properly connect\n",
        "    # the core to sequences.\n",
        "    \n",
        "    # self._core is an RNN, as before\n",
        "    # self._embed is the mapping from one-hot x_t to input of the RNN\n",
        "    # self._output_module is the mapping from RNN output to logits over chars\n",
        "    \n",
        "    ################\n",
        "    #   YOUR CODE  #\n",
        "    ################\n",
        "\n",
        "    return output_sequence_logits, final_state\n",
        "\n",
        "  @snt.reuse_variables\n",
        "  def generate_string(self, initial_logits, initial_state, sequence_length):\n",
        "    \"\"\"Builds sub-graph to generate a string, sampled from the model.\n",
        "    Args:\n",
        "      initial_logits: Starting logits to sample from.\n",
        "      initial_state: Starting state for the RNN core.\n",
        "      sequence_length: Number of characters to sample.\n",
        "    Returns:\n",
        "      A Tensor of characters, with dimensions `[sequence_length, batch_size,\n",
        "      output_size]`.\n",
        "      A Tensor of activities of hidden neurons\n",
        "    \"\"\"\n",
        "    \n",
        "    current_logits = initial_logits\n",
        "    current_state = initial_state\n",
        "    activations = None\n",
        "    \n",
        "    generated_letters = []\n",
        "    for _ in range(sequence_length):\n",
        "      # Sample a character index from distribution.\n",
        "      char_index = tf.squeeze(tf.multinomial(current_logits, 1))\n",
        "      char_one_hot = tf.one_hot(char_index, self._output_size, 1.0, 0.0)\n",
        "      \n",
        "      generated_letters.append(char_one_hot)\n",
        "      \n",
        "      # Feed character back into the deep_lstm.\n",
        "      gen_out_seq, current_state = self._core(\n",
        "          self._embed(char_one_hot),\n",
        "          current_state)\n",
        "      current_logits = self._output_module(gen_out_seq)\n",
        "      \n",
        "    generated_string = tf.stack(generated_letters)\n",
        "    \n",
        "    # Expose activations as a stacked Tensor of RNN states\n",
        "    ################\n",
        "    #   YOUR CODE  #\n",
        "    ################\n",
        "    # This part requires modifying this method, rather than just adding code\n",
        "    \n",
        "    return generated_string, activations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "rMUp4v2nE7vr",
        "colab": {}
      },
      "source": [
        "#@title TextModel - solution\n",
        "\n",
        "class TextModel(snt.AbstractModule):\n",
        "  \"\"\"A deep RNN model, for use on the Tiny Shakespeare dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, num_hidden, depth, output_size, embedding_size=32,\n",
        "               rnn_core=snt.LSTM, name=\"text_model\"):\n",
        "    \"\"\"Constructs a `TextModel`.\n",
        "    Args:\n",
        "      num_hidden: Number of hidden units in each LSTM layer.\n",
        "      depth: Number of RNN layers.\n",
        "      output_size: Size of the output layer on top of the DeepRNN.\n",
        "      rnn_core: Reference to an RNNCore class such as LSTM.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "\n",
        "    super(TextModel, self).__init__(name=name)\n",
        "\n",
        "    self._num_hidden = num_hidden\n",
        "    self._depth = depth\n",
        "    self._output_size = output_size\n",
        "    self._rnn_core = rnn_core    \n",
        "    self._embed = snt.nets.MLP([embedding_size], activate_final=True)\n",
        "\n",
        "    with self._enter_variable_scope():\n",
        "      self._output_module = snt.Linear(self._output_size, name=\"linear_output\")\n",
        "      self._subcores = [\n",
        "          self._rnn_core(self._num_hidden, name=\"rnn_{}\".format(i))\n",
        "          for i in range(self._depth)\n",
        "      ]    \n",
        "      self._core = snt.DeepRNN(self._subcores,\n",
        "                               skip_connections=False,\n",
        "                               name=\"deep_rnn\")\n",
        "\n",
        "  def _build(self, one_hot_input_sequence):\n",
        "    \"\"\"Builds the deep RNN model sub-graph.\n",
        "    Args:\n",
        "      one_hot_input_sequence: A Tensor with the input sequence encoded as a\n",
        "        one-hot representation. Its dimensions should be `[truncation_length,\n",
        "        batch_size, output_size]`.\n",
        "    Returns:\n",
        "      Tuple of the Tensor of output logits for the batch, with dimensions\n",
        "      `[truncation_length, batch_size, output_size]`, and the\n",
        "      final state of the unrolled core,.\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = one_hot_input_sequence.get_shape()\n",
        "    batch_size = input_shape[1]\n",
        "\n",
        "    batch_embed_module = snt.BatchApply(self._embed)\n",
        "    input_sequence = one_hot_input_sequence\n",
        "    input_sequence = batch_embed_module(input_sequence)\n",
        "\n",
        "    initial_state = self._core.initial_state(batch_size)\n",
        "\n",
        "    rnn_input_sequence = tf.unstack(input_sequence)\n",
        "    output, final_state = tf.contrib.rnn.static_rnn(\n",
        "        cell=self._core,\n",
        "        inputs=rnn_input_sequence,\n",
        "        initial_state=initial_state)\n",
        "    output_sequence = tf.stack(output)\n",
        "\n",
        "    batch_output_module = snt.BatchApply(self._output_module)\n",
        "    output_sequence_logits = batch_output_module(output_sequence)\n",
        "\n",
        "    return output_sequence_logits, final_state\n",
        "\n",
        "  @snt.reuse_variables\n",
        "  def generate_string(self, initial_logits, initial_state, sequence_length):\n",
        "    \"\"\"Builds sub-graph to generate a string, sampled from the model.\n",
        "    Args:\n",
        "      initial_logits: Starting logits to sample from.\n",
        "      initial_state: Starting state for the RNN core.\n",
        "      sequence_length: Number of characters to sample.\n",
        "    Returns:\n",
        "      A Tensor of characters, with dimensions `[sequence_length, batch_size,\n",
        "      output_size]`.\n",
        "      A Tensor of activities of hidden neurons\n",
        "    \"\"\"\n",
        "\n",
        "    current_logits = initial_logits\n",
        "    current_state = initial_state\n",
        "    activations = []\n",
        "    \n",
        "    generated_letters = []\n",
        "    for _ in range(sequence_length):\n",
        "      # Sample a character index from distribution.\n",
        "      char_index = tf.squeeze(tf.multinomial(current_logits, 1))\n",
        "      char_one_hot = tf.one_hot(char_index, self._output_size, 1.0, 0.0)\n",
        "      \n",
        "      generated_letters.append(char_one_hot)\n",
        "      \n",
        "      # Feed character back into the deep_lstm.\n",
        "      gen_out_seq, current_state = self._core(\n",
        "          self._embed(char_one_hot),\n",
        "          current_state)\n",
        "      current_logits = self._output_module(gen_out_seq)\n",
        "      \n",
        "      activations.append(current_state)\n",
        "\n",
        "    generated_string = tf.stack(generated_letters)\n",
        "    activations = tf.stack(activations)\n",
        "\n",
        "    return generated_string, activations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "R7CVRaUW5nKU",
        "colab": {}
      },
      "source": [
        "#@title Building entire graph\n",
        "\n",
        "def build_graph(depth=3, batch_size=32, num_hidden=128,\n",
        "                truncation_length=64, sample_length=1000, max_grad_norm=5,\n",
        "                initial_learning_rate=0.1, reduce_learning_rate_multiplier=0.1,\n",
        "                optimizer_epsilon=0.01, rnn_core=snt.LSTM):\n",
        "\n",
        "  # Get datasets.\n",
        "  dataset_train = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"train\",\n",
        "      random=False,\n",
        "      name=\"shake_train\")\n",
        "\n",
        "  dataset_valid = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"valid\",\n",
        "      random=False,\n",
        "      name=\"shake_valid\")\n",
        "\n",
        "  dataset_test = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"test\",\n",
        "      random=False,\n",
        "      name=\"shake_test\")\n",
        "\n",
        "  # Define model.\n",
        "  model = TextModel(\n",
        "      num_hidden=num_hidden,\n",
        "      depth=depth,\n",
        "      output_size=dataset_valid.vocab_size,\n",
        "      rnn_core=rnn_core)\n",
        "\n",
        "  # Get the training loss.\n",
        "  train_input_sequence, train_target_sequence = dataset_train()\n",
        "  train_output_sequence_logits, train_final_state = model(train_input_sequence)  \n",
        "  train_loss = dataset_train.cost(train_output_sequence_logits,\n",
        "                                  train_target_sequence)\n",
        "\n",
        "  # Get the validation loss.\n",
        "  valid_input_sequence, valid_target_sequence = dataset_valid()\n",
        "  valid_output_sequence_logits, _ = model(valid_input_sequence)\n",
        "  valid_loss = dataset_valid.cost(valid_output_sequence_logits,\n",
        "                                  valid_target_sequence)\n",
        "\n",
        "  # Get the test loss.\n",
        "  test_input_sequence, test_target_sequence = dataset_test()\n",
        "  test_output_sequence_logits, _ = model(test_input_sequence) \n",
        "  test_loss = dataset_test.cost(test_output_sequence_logits,\n",
        "                                test_target_sequence)\n",
        "\n",
        "  # Build graph to sample some strings during training.\n",
        "  initial_logits = train_output_sequence_logits[truncation_length - 1]\n",
        "  train_generated_string, activations = model.generate_string(\n",
        "      initial_logits=initial_logits,\n",
        "      initial_state=train_final_state,\n",
        "      sequence_length=sample_length)\n",
        "  \n",
        "  # Set up global norm clipping of gradients.\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  \n",
        "  raw_grads = tf.gradients(train_loss, trainable_variables)\n",
        "  if max_grad_norm is not None:\n",
        "    grads, _ = tf.clip_by_global_norm(raw_grads, max_grad_norm)\n",
        "  else:\n",
        "    grads = raw_grads\n",
        "\n",
        "  # Get learning rate and define annealing.\n",
        "  learning_rate = tf.get_variable(\n",
        "      \"learning_rate\",\n",
        "      shape=[],\n",
        "      dtype=tf.float32,\n",
        "      initializer=tf.constant_initializer(initial_learning_rate),\n",
        "      trainable=False)\n",
        "  \n",
        "  reduce_learning_rate = learning_rate.assign(\n",
        "      learning_rate * reduce_learning_rate_multiplier)\n",
        "\n",
        "  # Get training step counter.\n",
        "  global_step = tf.get_variable(\n",
        "      name=\"global_step\",\n",
        "      shape=[],\n",
        "      dtype=tf.int64,\n",
        "      initializer=tf.zeros_initializer(),\n",
        "      trainable=False,\n",
        "      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
        "\n",
        "  # Define optimizer and training step.\n",
        "  optimizer = tf.train.AdamOptimizer(\n",
        "      learning_rate)\n",
        "  \n",
        "  train_step = optimizer.apply_gradients(\n",
        "      zip(grads, trainable_variables),\n",
        "      global_step=global_step)\n",
        "\n",
        "  graph_tensors = {\n",
        "      \"train_loss\": train_loss,\n",
        "      \"valid_loss\": valid_loss,\n",
        "      \"test_loss\": test_loss,\n",
        "      \"train_generated_string\": train_generated_string,\n",
        "      \"reduce_learning_rate\": reduce_learning_rate,\n",
        "      \"global_step\": global_step,\n",
        "      \"train_step\": train_step,\n",
        "      \"raw_gradients\": raw_grads,\n",
        "      \"activations\": activations\n",
        "  }\n",
        "\n",
        "  # Return dataset_train for translation to human readable text.\n",
        "  return graph_tensors, dataset_train\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "BwTVV-km6cOz",
        "colab": {}
      },
      "source": [
        "#@title Run experiment\n",
        "NUM_TRAINING_ITERATIONS = 2000 #@param\n",
        "REPORT_INTERVAL = 500 #@param\n",
        "REDUCE_LEARNING_RATE_INTERVAL = 1500 #@param\n",
        "\n",
        "DEPTH = 2 #@param\n",
        "BATCH_SIZE = 64 #@param\n",
        "NUM_HIDDEN = 128 #@param\n",
        "TRUNCATION_LENGTH = 64 #@param Sequence size for training.\n",
        "SAMPLE_LENGTH = 500 #@param Sequence size for sampling.\n",
        "MAX_GRAD_NORM = None #@param Gradient clipping norm limit.\n",
        "LEARNING_RATE = 0.01 #@param Optimizer learning rate.\n",
        "REDUCE_LEARNING_RATE_MULTIPLIER = 0.1 #@param Learning rate is multiplied by this when reduced.\n",
        "OPTIMIZER_EPSILON = 1e-8 #@param Adam epsilon\n",
        "\n",
        "RNN_CORE = \"snt.LSTM\" #@param ['RNN', 'snt.LSTM', 'snt.GRU', 'ExposedLSTM']\n",
        "RNN_CORE = eval(RNN_CORE)\n",
        "\n",
        "TESTING_ITERATIONS = 1000 #@param\n",
        "\n",
        "\n",
        "print(\"Building the graph\")\n",
        "print()\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "graph_tensors, dataset_train = build_graph(\n",
        "    depth=DEPTH, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_hidden=NUM_HIDDEN,\n",
        "    truncation_length=TRUNCATION_LENGTH,\n",
        "    sample_length=SAMPLE_LENGTH,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    initial_learning_rate=LEARNING_RATE,\n",
        "    reduce_learning_rate_multiplier=REDUCE_LEARNING_RATE_MULTIPLIER,\n",
        "    optimizer_epsilon=OPTIMIZER_EPSILON,\n",
        "    rnn_core=RNN_CORE)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Train the network.\n",
        "print(\"Training starts...\")\n",
        "print()\n",
        "\n",
        "sess = tf.train.MonitoredSession()\n",
        "sess.run(init)\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for train_iteration in range(NUM_TRAINING_ITERATIONS):\n",
        "  if train_iteration % REPORT_INTERVAL == 1:\n",
        "    train_loss_v, valid_loss_v, _, raw_gradients = sess.run(\n",
        "        (graph_tensors[\"train_loss\"],\n",
        "         graph_tensors[\"valid_loss\"],\n",
        "         graph_tensors[\"train_step\"],\n",
        "         graph_tensors[\"raw_gradients\"]))\n",
        "\n",
        "    train_generated_string_v = sess.run(\n",
        "        graph_tensors[\"train_generated_string\"])\n",
        "\n",
        "    train_generated_string_human = dataset_train.to_human_readable(\n",
        "        (train_generated_string_v, 0), False, indices=[0], pretify=True)\n",
        "\n",
        "    print(\"%d: Training loss %f. Validation loss %f.\" % (\n",
        "                    train_iteration,\n",
        "                    train_loss_v,\n",
        "                    valid_loss_v))\n",
        "\n",
        "    print(\"Gradient norm: %f\" % np.linalg.norm(np.concatenate(\n",
        "        [g.reshape(-1) for g in raw_gradients])))\n",
        "    print()\n",
        "    print(train_generated_string_human)\n",
        "    print()\n",
        "    \n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, c='b', label='Train')\n",
        "    plt.plot(valid_losses, c='g', label='Valid')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  else:\n",
        "    train_loss_v, valid_loss_v, _ = sess.run((graph_tensors[\"train_loss\"],\n",
        "                                              graph_tensors[\"valid_loss\"],\n",
        "                                              graph_tensors[\"train_step\"]))\n",
        "    \n",
        "  train_losses.append(train_loss_v)\n",
        "  valid_losses.append(valid_loss_v)\n",
        "\n",
        "\n",
        "  if (train_iteration + 1) % REDUCE_LEARNING_RATE_INTERVAL == 0:\n",
        "    sess.run(graph_tensors[\"reduce_learning_rate\"])\n",
        "    print(\"Reducing learning rate.\")\n",
        "\n",
        "test_losses = []    \n",
        "for k in range(TESTING_ITERATIONS):    \n",
        "  test_loss_v = sess.run(graph_tensors[\"test_loss\"])\n",
        "  test_losses.append(test_loss_v)\n",
        "  \n",
        "print(\"Test loss %f\" % np.mean(test_losses))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5_uGGyiZyI3w"
      },
      "source": [
        "## Ex 3.1   Analysis of single neurons and gates\n",
        "\n",
        "We will now look at the individual activations of neurons in a Recurrent network. For this to work, you need to have completed the previous exercise in which you expose the network activations, as well as train a model.\n",
        "\n",
        "For a similar analysis, see [this paper](https://arxiv.org/pdf/1506.02078.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "5gU06mESEcoT",
        "colab": {}
      },
      "source": [
        "#@title String plot function\n",
        "\n",
        "def string_plot(chars, values, title=None):\n",
        "  \"\"\"\n",
        "  Given a string \"chars\" and a vector of numbers \"values\" of the same length\n",
        "  displays the string, using \"|\" as EOL symbol, and colors each character\n",
        "  background using corresponding value in values\n",
        "  \"\"\"\n",
        "  \n",
        "  assert len(chars) == len(values)\n",
        "  \n",
        "  lines = []\n",
        "  line = \"\"\n",
        "  for char in chars:\n",
        "    if char != '|':\n",
        "      line += char\n",
        "    else:\n",
        "      line += \" \"\n",
        "      lines.append(line)\n",
        "      line = \"\"\n",
        "  lines.append(line)\n",
        "  \n",
        "  height = len(lines) \n",
        "  width = max(map(len, lines))\n",
        "    \n",
        "  data = np.zeros((height, width))\n",
        "  data[:,:] = np.nan\n",
        "  \n",
        "  pos = 0\n",
        "  for lid, line in enumerate(lines):\n",
        "    data[lid, :len(line)] = values[pos: pos+len(line)]\n",
        "    pos = pos+len(line)\n",
        "    \n",
        "  assert pos == len(values)\n",
        "    \n",
        "  plt.figure(figsize=(width * 0.3, height * 0.3))\n",
        "  plt.title(title)\n",
        "  plt.imshow(data.reshape((height, width)), interpolation='none',\n",
        "             cmap='Reds', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "  \n",
        "  for lid, line in enumerate(lines):\n",
        "    for cid, char in enumerate(line):\n",
        "      plt.text(cid-0.2,lid+0.2,char,color='k',fontsize=9)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "9sy-fC04-gXN",
        "colab": {}
      },
      "source": [
        "#@title Generate some new text and corresponding activations\n",
        "\n",
        "text, activations = sess.run([\n",
        "    graph_tensors['train_generated_string'],\n",
        "    graph_tensors['activations']\n",
        "])\n",
        "\n",
        "\n",
        "print(dataset_train.to_human_readable((text, 0), False, indices=[0],\n",
        "                                      pretify=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "JIoafi_KDJ3g",
        "colab": {}
      },
      "source": [
        "#@title Display activities of random neurons\n",
        "\n",
        "NUMBER_OF_NEURONS = 10 #@param\n",
        "STATE_DIM = 1 #@param \n",
        "\n",
        "if len(activations.shape) == 5:\n",
        "  selected_activations = activations[:, :, STATE_DIM, 0, :]\n",
        "else:\n",
        "  selected_activations = activations[:, :, 0, :]\n",
        "\n",
        "chars = list(dataset_train.to_human_readable(\n",
        "        (text, 0), False, indices=[0]))\n",
        "\n",
        "neurons_ids = np.random.choice(range(activations.shape[-1]), \n",
        "                               min(NUMBER_OF_NEURONS, activations.shape[-1]),\n",
        "                               replace=False)\n",
        "\n",
        "for layer_id in reversed(range(selected_activations.shape[1])): \n",
        "  values = selected_activations[:, layer_id, :].T\n",
        "  for neuron in sorted(neurons_ids):\n",
        "    string_plot(chars, \n",
        "                values[neuron],\n",
        "                title='Layer %d, Neuron %d, State %d' \n",
        "                % (layer_id, neuron, STATE_DIM))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lTOoND9nNGyp"
      },
      "source": [
        "### What kind of neurons can we expect to find?\n",
        "\n",
        "* Lots of counting neurons (their activity is just growing/decreasing independently from input)\n",
        "* Names neuron - activates around names of people in the play, such as HAMLET: or JOHN OF GAUNT:\n",
        "* Line width neuron - with activity proportional to the length of the current line (number of charaters since last \"|\")\n",
        "* Paragraph length neuron - activity proportional to the length of the paragraph in lines\n",
        "* Special character neurons - such as coding for probability of generating \":\"\n",
        "* Many, many mixtures of the above\n",
        "\n",
        "Note, that if neurons like these do not appear it does not mean that network does not \"know\" these elements. Highly discriminative, single neuron decoupling is not something neural networks are trained to do, it is just an empirical observation, shared across many domains (cat neurons in visual classifiers etc.). Knowledge can be represented in many other ways, in particular the fact that it is represented in a single neuron does not mean network does not have a distributed \"backup\" of the same knowledge somewhere else.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XtOHP2haXQ9F"
      },
      "source": [
        "## Ex 3.2   Analysis of the state dynamics\n",
        "\n",
        "In this exercise, we will visualize the activations in a different way, by projecting them to 2 dimensions, via dimensionality reduction. \n",
        "\n",
        "When using different projection techniques, you willl see different results. For example, PCA will display the directions with most variance in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "uksDGFBp69Kz",
        "colab": {}
      },
      "source": [
        "#@title Compute 2D projection of the hidden state\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, Isomap\n",
        "\n",
        "projector = 'PCA' #@param ['PCA', 'TSNE', 'Isomap']\n",
        "projector_fun = eval(projector)\n",
        "\n",
        "kwargs = {\n",
        "    'TSNE': {'perplexity': 5},\n",
        "    'PCA': {},\n",
        "    'Isomap': {}\n",
        "}\n",
        "\n",
        "values = selected_activations.reshape(selected_activations.shape[0], -1)\n",
        "projector = projector_fun(n_components=2, **kwargs[projector])\n",
        "\n",
        "values_2d = projector.fit_transform(values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "1_iOk3TDDyBM",
        "colab": {}
      },
      "source": [
        "#@title Data plotter\n",
        "\n",
        "splits = {\n",
        "    'nothing': ('$', ),\n",
        "    'lines': ('|', ),\n",
        "    'paragraphs': ('|', '|', ),\n",
        "}\n",
        "\n",
        "split_criterion = 'lines' #@param ['nothing', 'lines', 'paragraphs']\n",
        "\n",
        "parts = []\n",
        "part = \"\"\n",
        "for cid, char in enumerate(chars):\n",
        "  match = True\n",
        "  part += char\n",
        "  for k in range(min(len(chars)-cid, len(splits[split_criterion]))):\n",
        "    if chars[cid+k] != splits[split_criterion][k]:\n",
        "      match = False\n",
        "  if match:\n",
        "    parts.append(len(part))\n",
        "    part = \"\"\n",
        "    \n",
        "if len(part)>0:    \n",
        "  parts.append(len(part))    \n",
        "  \n",
        "\n",
        "plt.figure()\n",
        "plt.title('Hidden activities, colored according to %s' \n",
        "          % split_criterion.replace('_', ' '))\n",
        "current = 0\n",
        "for part_id, part in enumerate(parts):\n",
        "  plt.plot(values_2d[current:current+part,0],\n",
        "           values_2d[current:current+part,1],\n",
        "           label='Part %d' % part_id)\n",
        "  current += part\n",
        "  \n",
        "plt.legend(bbox_to_anchor=(1.2, 1.05))  \n",
        "plt.show()\n",
        "\n",
        "current = 0\n",
        "for part_id, part in enumerate(parts):\n",
        "  print('Part %d' % part_id)\n",
        "  print(dataset_train.to_human_readable((text[current:current+part], 0),\n",
        "                                              False, indices=[0],\n",
        "                                              pretify=True))\n",
        "  print()\n",
        "  \n",
        "  current += part\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oZmBGOnXxRsI"
      },
      "source": [
        "### So what am I looking at?\n",
        "\n",
        "2D projections of high-dimensional spaces are always loosing a lot of information, however the general structure can still be recovered. Here, one can see that both paragraph-splits and line-splits can be decoded by just looking at the dynamics of the hidden state, giving more insights into internals of an RNN. Note, that contrary to single-neuron analysis, here we are truly looking at the whole picture, thus what is observed is likely behind the dynamics of this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-7flnAITCk-L"
      },
      "source": [
        "## Ex 3.3   One step further (Optional)\n",
        "\n",
        "\n",
        "Now how about gates? Can we expose elements of our RNN to be able to visualise them too?\n",
        "\n",
        "This exercise entails implementing an LSTM core - including the gates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "yiEtZg_-W9c6",
        "colab": {}
      },
      "source": [
        "#@title Custom LSTM implementation, exposing its internals - Starting point\n",
        "\n",
        "import collections\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "\n",
        "ExposedLSTMState = collections.namedtuple(\"ExposedLSTMState\",\n",
        "                                          (\"hidden\", \"cell\", \n",
        "                                           \"forget_gate\", \"input_gate\",\n",
        "                                           ))\n",
        "\n",
        "\n",
        "class ExposedLSTM(snt.LSTM):\n",
        "  \n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(ExposedLSTM, self).__init__(*args, **kwargs)\n",
        "  \n",
        "  def _build(self, inputs, prev_state):\n",
        "\n",
        "    ################\n",
        "    #   YOUR CODE  #\n",
        "    ################\n",
        "    \n",
        "    return next_hidden, ExposedLSTMState(hidden=next_hidden,\n",
        "                                         cell=next_cell,\n",
        "                                         forget_gate=forget_mask,\n",
        "                                         input_gate=input_mask)\n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    \"\"\"Tuple of `tf.TensorShape`s indicating the size of state tensors.\"\"\"\n",
        "    return ExposedLSTMState(tf.TensorShape([self._hidden_state_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "ljHhbhd21f9u",
        "colab": {}
      },
      "source": [
        "#@title Custom LSTM implementation, exposing its internals - Solution\n",
        "\n",
        "import collections\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ExposedLSTMState = collections.namedtuple(\"ExposedLSTMState\",\n",
        "                                          (\"hidden\", \"cell\", \n",
        "                                           \"forget_gate\", \"input_gate\",\n",
        "                                           ))\n",
        "\n",
        "\n",
        "class ExposedLSTM(snt.LSTM):\n",
        "  \n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(ExposedLSTM, self).__init__(*args, **kwargs)\n",
        "  \n",
        "  def _build(self, inputs, prev_state):\n",
        "    # This method is copied from the parent class, snt.LSTM and modified\n",
        "    # only in terms of exposing a few variables\n",
        "\n",
        "    prev_hidden, prev_cell, _, _ = prev_state\n",
        "\n",
        "    self._create_gate_variables(inputs.get_shape(), inputs.dtype)\n",
        "\n",
        "    inputs_and_hidden = tf.concat([inputs, prev_hidden], 1)\n",
        "    gates = tf.matmul(inputs_and_hidden, self._w_xh)\n",
        "\n",
        "    if self._use_layer_norm:\n",
        "      gates = layer_norm.LayerNorm()(gates)\n",
        "\n",
        "    gates += self._b\n",
        "\n",
        "    # i = input_gate, j = next_input, f = forget_gate, o = output_gate\n",
        "    i, j, f, o = array_ops.split(value=gates, num_or_size_splits=4, axis=1)\n",
        "\n",
        "    if self._use_peepholes:  # diagonal connections\n",
        "      self._create_peephole_variables(inputs.dtype)\n",
        "      f += self._w_f_diag * prev_cell\n",
        "      i += self._w_i_diag * prev_cell\n",
        "\n",
        "    forget_mask = tf.sigmoid(f + self._forget_bias)\n",
        "    input_mask = tf.sigmoid(i)\n",
        "    \n",
        "    next_cell = forget_mask * prev_cell + input_mask * tf.tanh(j)\n",
        "    cell_output = next_cell\n",
        "    if self._use_peepholes:\n",
        "      cell_output += self._w_o_diag * cell_output\n",
        "    next_hidden = tf.tanh(cell_output) * tf.sigmoid(o)\n",
        "\n",
        "    if self._use_projection:\n",
        "      next_hidden = tf.matmul(next_hidden, self._w_h_projection)\n",
        "\n",
        "    return next_hidden, ExposedLSTMState(hidden=next_hidden,\n",
        "                                         cell=next_cell,\n",
        "                                         forget_gate=forget_mask,\n",
        "                                         input_gate=input_mask)\n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    \"\"\"Tuple of `tf.TensorShape`s indicating the size of state tensors.\"\"\"\n",
        "    return ExposedLSTMState(tf.TensorShape([self._hidden_state_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            tf.TensorShape([self._hidden_size]),\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z3F1MqH5Cowm"
      },
      "source": [
        "### What kind of neuron-gate interactions we can expect?\n",
        "\n",
        "This should further support previous findings - showing how gates are used to support specific neuronal functions. For example, can you find:\n",
        "\n",
        "* A \"lets start from scratch as this is a new line\" interaction?\n",
        "* A \"lets start from scratch as this is a new paragraph\" interaction?\n",
        "\n",
        "The search for these interpretable internals can be exhaustive, but they will emerge in nearly every run. "
      ]
    }
  ]
}